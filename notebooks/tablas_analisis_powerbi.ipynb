{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "744414df-0e19-44c9-9a7e-4e0f5b70f8ab",
   "metadata": {},
   "source": [
    "# Generaci√≥n de tablas para Power BI (modelo global y diagn√≥stico)\n",
    "\n",
    "Este notebook tiene como objetivo **materializar el output del modelo de devoluciones y del recomendador en tablas anal√≠ticas**, listas para ser consumidas directamente en Power BI.\n",
    "\n",
    "No se entrena ning√∫n modelo nuevo: aqu√≠ se **operacionaliza** lo ya construido, asegurando trazabilidad, coherencia y estabilidad entre datos, predicciones y m√©tricas de negocio.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Predicciones globales a nivel item\n",
    "\n",
    "A partir de los datos procesados del modelo (`X_train`, `X_test`) y sus √≠ndices asociados (`train_index`, `test_index`), se cargan:\n",
    "\n",
    "- las features finales usadas por el modelo,\n",
    "- el modelo XGBoost entrenado (`xgb_final.json`).\n",
    "\n",
    "Se generan predicciones de probabilidad de devoluci√≥n (`p_dev_global`) para **todo el universo hist√≥rico**, tanto train como test, y se construye una tabla √∫nica a nivel item:\n",
    "\n",
    "- `item_id`\n",
    "- `ticket_id`\n",
    "- `customer_id`\n",
    "- `fecha_compra`\n",
    "- `devuelto_real`\n",
    "- `p_dev_global`\n",
    "\n",
    "Esta tabla se guarda como:\n",
    "\n",
    "- `preds_global_item_level.csv`\n",
    "\n",
    "y constituye la **base del diagn√≥stico global de riesgo**.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Diagn√≥stico econ√≥mico global\n",
    "\n",
    "Las predicciones se enriquecen con informaci√≥n real de los items (`items_devoluciones_ajustadas`), incorporando:\n",
    "\n",
    "- precio neto,\n",
    "- coste de devoluci√≥n estimado,\n",
    "- variables de producto, cliente y log√≠stica.\n",
    "\n",
    "A partir de ello se calcula:\n",
    "\n",
    "expected_cost_global = p_dev_global √ó coste_devolucion\n",
    "\n",
    "Esto permite cuantificar, **antes de cualquier intervenci√≥n**, el coste esperado de devoluciones por item.\n",
    "\n",
    "El resultado se guarda como:\n",
    "\n",
    "- `items_global_diagnostico.csv`\n",
    "\n",
    "y sirve para:\n",
    "- an√°lisis exploratorio,\n",
    "- mapas de riesgo,\n",
    "- KPIs de coste esperado.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Agregaciones temporales para BI\n",
    "\n",
    "Para facilitar el an√°lisis temporal en Power BI, se generan tablas agregadas diarias:\n",
    "\n",
    "### Por canal\n",
    "- ventas\n",
    "- n√∫mero de tickets\n",
    "- items\n",
    "- clientes\n",
    "\n",
    "‚Üí `channel_daily.csv`\n",
    "\n",
    "### Por categor√≠a\n",
    "- ventas\n",
    "- tickets\n",
    "- items\n",
    "- clientes\n",
    "\n",
    "‚Üí `category_daily.csv`\n",
    "\n",
    "### Por cliente\n",
    "- presencia diaria del cliente\n",
    "\n",
    "‚Üí `customer_daily.csv`\n",
    "\n",
    "Estas tablas permiten construir dashboards de evoluci√≥n sin necesidad de c√°lculos complejos en BI.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Tabla item-level del modelo global (BI-friendly)\n",
    "\n",
    "Se construye una tabla a nivel item que combina:\n",
    "\n",
    "- identificadores (item, ticket, cliente),\n",
    "- variable objetivo real,\n",
    "- score del modelo (`p_dev_global`),\n",
    "- proxy de coste (precio neto),\n",
    "- coste esperado de devoluci√≥n.\n",
    "\n",
    "El resultado es:\n",
    "\n",
    "- `items_model_global.parquet / csv`\n",
    "\n",
    "Esta tabla est√° pensada como **fuente √∫nica de verdad** para an√°lisis de riesgo y coste del modelo global en Power BI.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Tabla enriquecida final (modelo + negocio)\n",
    "\n",
    "En un segundo paso, se genera una tabla **enriquecida**, uniendo:\n",
    "\n",
    "- predicciones del modelo global,\n",
    "- variables reales del item (producto, talla, cliente, log√≠stica),\n",
    "- coste de devoluci√≥n real,\n",
    "- fechas de compra y devoluci√≥n.\n",
    "\n",
    "El merge se realiza de forma defensiva:\n",
    "- validando claves,\n",
    "- controlando duplicados,\n",
    "- renombrando variables ambiguas (`devuelto_real`, `devuelto_items`).\n",
    "\n",
    "El output final es:\n",
    "\n",
    "- `items_model_global_enriched.parquet / csv`\n",
    "\n",
    "Esta es la **tabla principal para Power BI**, a nivel item, desde la que se construyen:\n",
    "- p√°ginas de diagn√≥stico,\n",
    "- an√°lisis por categor√≠a y producto,\n",
    "- contexto econ√≥mico del recomendador de tallas.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Relaci√≥n con el recomendador de tallas\n",
    "\n",
    "Este notebook es el **puente entre el modelo y el negocio**:\n",
    "\n",
    "- el modelo global aporta el riesgo base (`p_dev_global`),\n",
    "- el recomendador de tallas compara escenarios contra ese riesgo,\n",
    "- Power BI consume ambos para medir impacto.\n",
    "\n",
    "Gracias a esta estructura, es posible medir:\n",
    "\n",
    "- ahorro por item,\n",
    "- ahorro por intervenci√≥n,\n",
    "- share del coste total atacable por talla.\n",
    "\n",
    "Sin este paso, el recomendador no ser√≠a auditable ni explicable desde negocio.\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Conclusi√≥n\n",
    "\n",
    "Este notebook no a√±ade complejidad algor√≠tmica, pero es **cr√≠tico para producci√≥n**. Garantiza que:\n",
    "\n",
    "- las predicciones del modelo son reproducibles,\n",
    "- los KPIs econ√≥micos son coherentes,\n",
    "- el an√°lisis en Power BI est√° alineado con la l√≥gica del modelo.\n",
    "\n",
    "Es el punto donde el proyecto deja de ser ‚Äúmodelado‚Äù y pasa a ser **sistema anal√≠tico usable**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e929f5ef-a1fc-4077-b346-f5a7ee3c01bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.0.0\n"
     ]
    }
   ],
   "source": [
    "import pyarrow\n",
    "print(pyarrow.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c01d4c51-9d8d-4d61-8a2a-044658fe4fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "BASE = Path(\"data/processed/devoluciones\")\n",
    "\n",
    "# cargar datasets\n",
    "X_train = pd.read_parquet(BASE / \"X_train.parquet\")\n",
    "X_test  = pd.read_parquet(BASE / \"X_test.parquet\")\n",
    "\n",
    "train_idx = pd.read_parquet(BASE / \"train_index.parquet\")\n",
    "test_idx  = pd.read_parquet(BASE / \"test_index.parquet\")\n",
    "\n",
    "# cargar modelo\n",
    "model = xgb.XGBClassifier()\n",
    "model.load_model(\"modelos/devoluciones/xgb_final.json\")\n",
    "\n",
    "# predicciones\n",
    "train_idx[\"p_dev_global\"] = model.predict_proba(X_train)[:, 1]\n",
    "test_idx[\"p_dev_global\"]  = model.predict_proba(X_test)[:, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "645b1e47-c0a0-4515-bd04-1dcdf969778a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fecha_compra</th>\n",
       "      <th>ticket_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>devuelto</th>\n",
       "      <th>p_dev_global</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-08-01</td>\n",
       "      <td>T000001</td>\n",
       "      <td>T000001-001</td>\n",
       "      <td>C000001</td>\n",
       "      <td>0</td>\n",
       "      <td>0.421737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-08-01</td>\n",
       "      <td>T000002</td>\n",
       "      <td>T000002-001</td>\n",
       "      <td>C000002</td>\n",
       "      <td>0</td>\n",
       "      <td>0.445007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-08-01</td>\n",
       "      <td>T000003</td>\n",
       "      <td>T000003-001</td>\n",
       "      <td>C000003</td>\n",
       "      <td>0</td>\n",
       "      <td>0.310177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-08-01</td>\n",
       "      <td>T000003</td>\n",
       "      <td>T000003-002</td>\n",
       "      <td>C000003</td>\n",
       "      <td>1</td>\n",
       "      <td>0.395241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-08-01</td>\n",
       "      <td>T000005</td>\n",
       "      <td>T000005-001</td>\n",
       "      <td>C000004</td>\n",
       "      <td>0</td>\n",
       "      <td>0.341808</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  fecha_compra ticket_id      item_id customer_id  devuelto  p_dev_global\n",
       "0   2017-08-01   T000001  T000001-001     C000001         0      0.421737\n",
       "1   2017-08-01   T000002  T000002-001     C000002         0      0.445007\n",
       "2   2017-08-01   T000003  T000003-001     C000003         0      0.310177\n",
       "3   2017-08-01   T000003  T000003-002     C000003         1      0.395241\n",
       "4   2017-08-01   T000005  T000005-001     C000004         0      0.341808"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_global = pd.concat(\n",
    "    [train_idx, test_idx],\n",
    "    axis=0,\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "preds_global.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64e3dcda-0ded-4ce2-b819-1007ea0817a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_global.to_csv(\n",
    "    \"data/bi/preds_global_item_level.csv\",\n",
    "    index=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "557b42d3-9731-4c42-8f59-69b713dfa8d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filas: 905445\n",
      "Columnas: 6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fecha_compra</th>\n",
       "      <th>ticket_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>devuelto</th>\n",
       "      <th>p_dev_global</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-08-01</td>\n",
       "      <td>T000001</td>\n",
       "      <td>T000001-001</td>\n",
       "      <td>C000001</td>\n",
       "      <td>0</td>\n",
       "      <td>0.421737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-08-01</td>\n",
       "      <td>T000002</td>\n",
       "      <td>T000002-001</td>\n",
       "      <td>C000002</td>\n",
       "      <td>0</td>\n",
       "      <td>0.445007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-08-01</td>\n",
       "      <td>T000003</td>\n",
       "      <td>T000003-001</td>\n",
       "      <td>C000003</td>\n",
       "      <td>0</td>\n",
       "      <td>0.310177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-08-01</td>\n",
       "      <td>T000003</td>\n",
       "      <td>T000003-002</td>\n",
       "      <td>C000003</td>\n",
       "      <td>1</td>\n",
       "      <td>0.395241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-08-01</td>\n",
       "      <td>T000005</td>\n",
       "      <td>T000005-001</td>\n",
       "      <td>C000004</td>\n",
       "      <td>0</td>\n",
       "      <td>0.341808</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  fecha_compra ticket_id      item_id customer_id  devuelto  p_dev_global\n",
       "0   2017-08-01   T000001  T000001-001     C000001         0      0.421737\n",
       "1   2017-08-01   T000002  T000002-001     C000002         0      0.445007\n",
       "2   2017-08-01   T000003  T000003-001     C000003         0      0.310177\n",
       "3   2017-08-01   T000003  T000003-002     C000003         1      0.395241\n",
       "4   2017-08-01   T000005  T000005-001     C000004         0      0.341808"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Filas:\", preds_global.shape[0])\n",
    "print(\"Columnas:\", preds_global.shape[1])\n",
    "preds_global.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "135bea8a-4378-4abe-a7e5-d4c06f3fb825",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_global.to_csv(\n",
    "    \"data/bi/preds_global_item_level.csv\",\n",
    "    index=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcc4421-45c2-4cc8-b7fc-c5c325908243",
   "metadata": {},
   "source": [
    "# tabla maestra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f31e497d-a46a-4e2d-b69c-96c143c81fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PEDRO\\AppData\\Local\\Temp\\ipykernel_12044\\1013978717.py:3: DtypeWarning: Columns (3,5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  items = pd.read_csv(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "items = pd.read_csv(\n",
    "    \"data/items_devoluciones_ajustadas.csv\",\n",
    "    encoding=\"utf-8-sig\",\n",
    "    parse_dates=[\"fecha_item\", \"fecha_devolucion\"]\n",
    ")\n",
    "\n",
    "preds = pd.read_csv(\n",
    "    \"data/bi/preds_global_item_level.csv\",\n",
    "    encoding=\"utf-8-sig\",\n",
    "    parse_dates=[\"fecha_compra\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0077e5e1-b0c2-446c-9107-1e7c34d0d5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_global = items.merge(\n",
    "    preds[[\"item_id\", \"p_dev_global\"]],\n",
    "    on=\"item_id\",\n",
    "    how=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b7050ea-abb2-4be3-a35d-03f42e6c4724",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_global[\"expected_cost_global\"] = (\n",
    "    df_global[\"p_dev_global\"] * df_global[\"coste_devolucion\"]\n",
    ")\n",
    "\n",
    "df_global[\"venta_neta\"] = df_global[\"precio_neto_unit\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea65cc45-f0cf-412f-9587-b418e5515273",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_global.to_csv(\n",
    "    \"data/bi/items_global_diagnostico.csv\",\n",
    "    index=False,\n",
    "    encoding=\"utf-8-sig\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf5e02d-1dc6-40f9-b7c9-2e1598475dca",
   "metadata": {},
   "source": [
    "# tablas bien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11a47391-6458-4cb3-a03f-af5dce2b153e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(905445, 35)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dia</th>\n",
       "      <th>ticket_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>venta_neta</th>\n",
       "      <th>canal_bi</th>\n",
       "      <th>categoria</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-08-01</td>\n",
       "      <td>T000001</td>\n",
       "      <td>T000001-001</td>\n",
       "      <td>C000001</td>\n",
       "      <td>90.25</td>\n",
       "      <td>online</td>\n",
       "      <td>Abrigo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-08-01</td>\n",
       "      <td>T000002</td>\n",
       "      <td>T000002-001</td>\n",
       "      <td>C000002</td>\n",
       "      <td>95.00</td>\n",
       "      <td>online</td>\n",
       "      <td>Abrigo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-08-01</td>\n",
       "      <td>T000003</td>\n",
       "      <td>T000003-001</td>\n",
       "      <td>C000003</td>\n",
       "      <td>26.00</td>\n",
       "      <td>online</td>\n",
       "      <td>Camiseta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-08-01</td>\n",
       "      <td>T000003</td>\n",
       "      <td>T000003-002</td>\n",
       "      <td>C000003</td>\n",
       "      <td>60.00</td>\n",
       "      <td>online</td>\n",
       "      <td>Sudadera</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-02-14</td>\n",
       "      <td>T000004</td>\n",
       "      <td>T000004-001</td>\n",
       "      <td>C000003</td>\n",
       "      <td>51.00</td>\n",
       "      <td>online</td>\n",
       "      <td>Sudadera</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         dia ticket_id      item_id customer_id  venta_neta canal_bi categoria\n",
       "0 2017-08-01   T000001  T000001-001     C000001       90.25   online    Abrigo\n",
       "1 2017-08-01   T000002  T000002-001     C000002       95.00   online    Abrigo\n",
       "2 2017-08-01   T000003  T000003-001     C000003       26.00   online  Camiseta\n",
       "3 2017-08-01   T000003  T000003-002     C000003       60.00   online  Sudadera\n",
       "4 2021-02-14   T000004  T000004-001     C000003       51.00   online  Sudadera"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "df = pd.read_csv(\n",
    "    \"data/bi/items_global_diagnostico.csv\",\n",
    "    parse_dates=[\"fecha_item\", \"fecha_devolucion\"],\n",
    "    low_memory=False\n",
    ")\n",
    "\n",
    "# nos quedamos con la fecha d√≠a\n",
    "df[\"dia\"] = df[\"fecha_item\"].dt.floor(\"D\")\n",
    "\n",
    "# normalizar canal/provincia por si acaso\n",
    "df[\"canal_bi\"] = (df[\"canal_norm\"] if \"canal_norm\" in df.columns else df[\"canal\"]).astype(str).str.lower().str.strip()\n",
    "df[\"provincia_bi\"] = (df[\"provincia_norm\"] if \"provincia_norm\" in df.columns else df[\"provincia\"]).astype(str).str.lower().str.strip()\n",
    "\n",
    "print(df.shape)\n",
    "df[[\"dia\",\"ticket_id\",\"item_id\",\"customer_id\",\"venta_neta\",\"canal_bi\",\"categoria\"]].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "435d747c-7ee4-48c1-9a0f-5dfb460c8744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardado channel_daily: (4352, 6)\n"
     ]
    }
   ],
   "source": [
    "channel_daily = (\n",
    "    df.groupby([\"dia\",\"canal_bi\"], dropna=False)\n",
    "      .agg(\n",
    "          ventas=(\"venta_neta\",\"sum\"),\n",
    "          tickets=(\"ticket_id\",\"nunique\"),\n",
    "          items=(\"item_id\",\"count\"),\n",
    "          clientes=(\"customer_id\",\"nunique\"),\n",
    "      )\n",
    "      .reset_index()\n",
    "      .rename(columns={\"canal_bi\":\"canal\"})\n",
    ")\n",
    "\n",
    "channel_daily.to_csv(\"data/bi/channel_daily.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "print(\"Guardado channel_daily:\", channel_daily.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f730cd4-73d9-4bcd-ad88-28c5dd646c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardado category_daily: (21512, 6)\n"
     ]
    }
   ],
   "source": [
    "category_daily = (\n",
    "    df.groupby([\"dia\",\"categoria\"], dropna=False)\n",
    "      .agg(\n",
    "          ventas=(\"venta_neta\",\"sum\"),\n",
    "          tickets=(\"ticket_id\",\"nunique\"),\n",
    "          items=(\"item_id\",\"count\"),\n",
    "          clientes=(\"customer_id\",\"nunique\"),\n",
    "      )\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "category_daily.to_csv(\"data/bi/category_daily.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "print(\"Guardado category_daily:\", category_daily.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c711f7dd-7db7-4861-bde0-af300a18e9e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardado customer_daily: (440443, 2)\n"
     ]
    }
   ],
   "source": [
    "customer_daily = (\n",
    "    df[[\"dia\",\"customer_id\"]]\n",
    "      .dropna()\n",
    "      .drop_duplicates()\n",
    "      .sort_values([\"dia\",\"customer_id\"])\n",
    ")\n",
    "\n",
    "customer_daily.to_csv(\"data/bi/customer_daily.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "print(\"Guardado customer_daily:\", customer_daily.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64fbeea-3c35-476b-ac85-9c1885c7e19c",
   "metadata": {},
   "source": [
    "# tabla devoluciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7f2af3a-e71e-4d80-9c6e-82416046d431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Guardado modelos/devoluciones/feature_columns.json con 83 features\n",
      "‚úÖ items_model_global creado\n",
      "Parquet: data/bi\\items_model_global.parquet\n",
      "CSV: data/bi\\items_model_global.csv\n",
      "Shape final: (905445, 9)\n",
      "  fecha_compra ticket_id      item_id customer_id  devuelto_real  \\\n",
      "0   2017-08-01   T000001  T000001-001     C000001              0   \n",
      "1   2017-08-01   T000002  T000002-001     C000002              0   \n",
      "2   2017-08-01   T000003  T000003-001     C000003              0   \n",
      "\n",
      "   p_dev_global  precio_neto  coste_base_unit  expected_cost_global  \n",
      "0      0.421614        90.25            90.25             38.050665  \n",
      "1      0.442739        95.00            95.00             42.060214  \n",
      "2      0.307088        26.00            26.00              7.984283  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "# Rutas (ajusta si tu proyecto tiene otras carpetas)\n",
    "PATH_X_TRAIN = \"data/processed/devoluciones/X_train.parquet\"\n",
    "PATH_X_TEST  = \"data/processed/devoluciones/X_test.parquet\"\n",
    "PATH_I_TRAIN = \"data/processed/devoluciones/train_index.parquet\"\n",
    "PATH_I_TEST  = \"data/processed/devoluciones/test_index.parquet\"\n",
    "\n",
    "PATH_MODEL   = \"modelos/devoluciones/xgb_final.json\"\n",
    "\n",
    "OUT_DIR      = \"data/bi\"\n",
    "OUT_PARQUET  = os.path.join(OUT_DIR, \"items_model_global.parquet\")\n",
    "OUT_CSV      = os.path.join(OUT_DIR, \"items_model_global.csv\")\n",
    "\n",
    "FEATURES_JSON = \"modelos/devoluciones/feature_columns.json\"\n",
    "\n",
    "\n",
    "def _ensure_dir(path: str) -> None:\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "\n",
    "def _load_and_check():\n",
    "    X_train = pd.read_parquet(PATH_X_TRAIN)\n",
    "    X_test  = pd.read_parquet(PATH_X_TEST)\n",
    "\n",
    "    idx_train = pd.read_parquet(PATH_I_TRAIN)\n",
    "    idx_test  = pd.read_parquet(PATH_I_TEST)\n",
    "\n",
    "    # Check columnas iguales y en el mismo orden\n",
    "    if list(X_train.columns) != list(X_test.columns):\n",
    "        missing_in_test = set(X_train.columns) - set(X_test.columns)\n",
    "        missing_in_train = set(X_test.columns) - set(X_train.columns)\n",
    "        raise ValueError(\n",
    "            \"‚ùå X_train y X_test NO tienen exactamente las mismas columnas.\\n\"\n",
    "            f\"Missing in test: {missing_in_test}\\n\"\n",
    "            f\"Missing in train: {missing_in_train}\\n\"\n",
    "            \"Soluci√≥n: alinear columnas antes de predecir.\"\n",
    "        )\n",
    "\n",
    "    # Check tama√±os coinciden con index\n",
    "    if len(X_train) != len(idx_train):\n",
    "        raise ValueError(f\"‚ùå X_train ({len(X_train)}) y train_index ({len(idx_train)}) no coinciden en filas.\")\n",
    "    if len(X_test) != len(idx_test):\n",
    "        raise ValueError(f\"‚ùå X_test ({len(X_test)}) y test_index ({len(idx_test)}) no coinciden en filas.\")\n",
    "\n",
    "    return X_train, X_test, idx_train, idx_test\n",
    "\n",
    "\n",
    "def _predict_proba(model_path: str, X: pd.DataFrame) -> pd.Series:\n",
    "    booster = xgb.Booster()\n",
    "    booster.load_model(model_path)\n",
    "\n",
    "    dmat = xgb.DMatrix(X, feature_names=list(X.columns))\n",
    "    p = booster.predict(dmat)\n",
    "\n",
    "    # Por si acaso, convertir a Series con mismo √≠ndice\n",
    "    return pd.Series(p, index=X.index, name=\"p_dev_global\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    _ensure_dir(OUT_DIR)\n",
    "\n",
    "    X_train, X_test, idx_train, idx_test = _load_and_check()\n",
    "\n",
    "    # Guardar features para siempre (recomendado)\n",
    "    _ensure_dir(os.path.dirname(FEATURES_JSON))\n",
    "    with open(FEATURES_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(list(X_train.columns), f, ensure_ascii=False, indent=2)\n",
    "    print(f\"‚úÖ Guardado {FEATURES_JSON} con {X_train.shape[1]} features\")\n",
    "\n",
    "    # Predicciones\n",
    "    p_train = _predict_proba(PATH_MODEL, X_train)\n",
    "    p_test  = _predict_proba(PATH_MODEL, X_test)\n",
    "\n",
    "    # Construir tabla train y test con ids + predicciones\n",
    "    train_out = idx_train.copy()\n",
    "    train_out[\"p_dev_global\"] = p_train.values\n",
    "\n",
    "    test_out = idx_test.copy()\n",
    "    test_out[\"p_dev_global\"] = p_test.values\n",
    "\n",
    "    # Concatenar (esto es tu tabla item-level diagn√≥stica)\n",
    "    out = pd.concat([train_out, test_out], ignore_index=True)\n",
    "\n",
    "    # A√±adir coste base (para expected_cost)\n",
    "    # Usamos precio_neto como proxy de \"dinero en riesgo\"\n",
    "    # OJO: precio_neto est√° en X_train/X_test, no en index -> lo a√±adimos tambi√©n\n",
    "    # (para no perderlo, lo unimos en el mismo orden de filas)\n",
    "    precio_neto_train = X_train[\"precio_neto\"].reset_index(drop=True)\n",
    "    precio_neto_test  = X_test[\"precio_neto\"].reset_index(drop=True)\n",
    "    out[\"precio_neto\"] = pd.concat([precio_neto_train, precio_neto_test], ignore_index=True)\n",
    "\n",
    "    # Coste base unitario (defensivo: no negativos)\n",
    "    out[\"coste_base_unit\"] = out[\"precio_neto\"].clip(lower=0)\n",
    "\n",
    "    # Coste esperado\n",
    "    out[\"expected_cost_global\"] = out[\"p_dev_global\"] * out[\"coste_base_unit\"]\n",
    "\n",
    "    # Renombrar devuelto a devuelto_real (m√°s claro en BI)\n",
    "    if \"devuelto\" in out.columns:\n",
    "        out = out.rename(columns={\"devuelto\": \"devuelto_real\"})\n",
    "\n",
    "    # Reordenar columnas (BI-friendly)\n",
    "    cols_first = [\n",
    "        \"fecha_compra\",\n",
    "        \"ticket_id\",\n",
    "        \"item_id\",\n",
    "        \"customer_id\",\n",
    "        \"devuelto_real\",\n",
    "        \"p_dev_global\",\n",
    "        \"precio_neto\",\n",
    "        \"coste_base_unit\",\n",
    "        \"expected_cost_global\",\n",
    "    ]\n",
    "    cols_rest = [c for c in out.columns if c not in cols_first]\n",
    "    out = out[cols_first + cols_rest]\n",
    "\n",
    "    # Guardar\n",
    "    out.to_parquet(OUT_PARQUET, index=False)\n",
    "    out.to_csv(OUT_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    print(\"‚úÖ items_model_global creado\")\n",
    "    print(\"Parquet:\", OUT_PARQUET)\n",
    "    print(\"CSV:\", OUT_CSV)\n",
    "    print(\"Shape final:\", out.shape)\n",
    "    print(out.head(3))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f3b6d7c-8fdf-42bb-97df-2f9643e44f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÑ X_train.parquet\n",
      "Shape: (679083, 83)\n",
      "Columnas:\n",
      "['descuento', 'precio_neto', 'coste_bruto', 'margen', 'n_pedidos', 'n_items_comprados', 'altura_cm', 'peso_kg', 'anio_compra', 'mes_compra', 'edad_en_compra', 'antiguedad_cliente_dias', 'en_promocion', 'margen_relativo', 'desajuste_talla', 'desajuste_talla_abs', 'talla_extrema', 'bmi', 'compras_previas_cliente', 'devoluciones_previas_cliente', 'ratio_devoluciones_previas_cliente', 'ventas_previas_producto', 'devoluciones_previas_producto', 'ratio_devoluciones_previas_producto', 'precio_rel_cat', 'missing_provincia_cliente', 'missing_comunidad', 'missing_altura_cm', 'missing_peso_kg', 'missing_edad_en_compra', 'missing_antiguedad_cliente_dias', 'missing_desajuste_talla', 'missing_bmi', 'canal_online', 'comunidad_aragon', 'comunidad_asturias', 'comunidad_baleares', 'comunidad_canarias', 'comunidad_cantabria', 'comunidad_castilla y leon', 'comunidad_castilla-la mancha', 'comunidad_cataluna', 'comunidad_ceuta', 'comunidad_comunidad valenciana', 'comunidad_extremadura', 'comunidad_galicia', 'comunidad_la rioja', 'comunidad_madrid', 'comunidad_melilla', 'comunidad_murcia', 'comunidad_navarra', 'comunidad_pais vasco', 'comunidad_unknown', 'categoria_bufanda', 'categoria_calcetines', 'categoria_calzado', 'categoria_camisa', 'categoria_camiseta', 'categoria_cinturon', 'categoria_gorra', 'categoria_pantalon', 'categoria_sudadera', 'color_blk', 'color_blu', 'color_brn', 'color_gry', 'color_lme', 'color_nav', 'color_red', 'color_wht', 'talla_40', 'talla_41', 'talla_42', 'talla_43', 'talla_44', 'talla_45', 'talla_l', 'talla_m', 'talla_onesize', 'talla_s', 'talla_xl', 'talla_xs', 'temporada_compra_ss']\n",
      "\n",
      "üìÑ X_test.parquet\n",
      "Shape: (226362, 83)\n",
      "Columnas:\n",
      "['descuento', 'precio_neto', 'coste_bruto', 'margen', 'n_pedidos', 'n_items_comprados', 'altura_cm', 'peso_kg', 'anio_compra', 'mes_compra', 'edad_en_compra', 'antiguedad_cliente_dias', 'en_promocion', 'margen_relativo', 'desajuste_talla', 'desajuste_talla_abs', 'talla_extrema', 'bmi', 'compras_previas_cliente', 'devoluciones_previas_cliente', 'ratio_devoluciones_previas_cliente', 'ventas_previas_producto', 'devoluciones_previas_producto', 'ratio_devoluciones_previas_producto', 'precio_rel_cat', 'missing_provincia_cliente', 'missing_comunidad', 'missing_altura_cm', 'missing_peso_kg', 'missing_edad_en_compra', 'missing_antiguedad_cliente_dias', 'missing_desajuste_talla', 'missing_bmi', 'canal_online', 'comunidad_aragon', 'comunidad_asturias', 'comunidad_baleares', 'comunidad_canarias', 'comunidad_cantabria', 'comunidad_castilla y leon', 'comunidad_castilla-la mancha', 'comunidad_cataluna', 'comunidad_ceuta', 'comunidad_comunidad valenciana', 'comunidad_extremadura', 'comunidad_galicia', 'comunidad_la rioja', 'comunidad_madrid', 'comunidad_melilla', 'comunidad_murcia', 'comunidad_navarra', 'comunidad_pais vasco', 'comunidad_unknown', 'categoria_bufanda', 'categoria_calcetines', 'categoria_calzado', 'categoria_camisa', 'categoria_camiseta', 'categoria_cinturon', 'categoria_gorra', 'categoria_pantalon', 'categoria_sudadera', 'color_blk', 'color_blu', 'color_brn', 'color_gry', 'color_lme', 'color_nav', 'color_red', 'color_wht', 'talla_40', 'talla_41', 'talla_42', 'talla_43', 'talla_44', 'talla_45', 'talla_l', 'talla_m', 'talla_onesize', 'talla_s', 'talla_xl', 'talla_xs', 'temporada_compra_ss']\n",
      "\n",
      "üìÑ y_train.parquet\n",
      "Shape: (679083, 1)\n",
      "Columnas:\n",
      "['devuelto']\n",
      "\n",
      "üìÑ y_test.parquet\n",
      "Shape: (226362, 1)\n",
      "Columnas:\n",
      "['devuelto']\n",
      "\n",
      "üìÑ train_index.parquet\n",
      "Shape: (679083, 5)\n",
      "Columnas:\n",
      "['fecha_compra', 'ticket_id', 'item_id', 'customer_id', 'devuelto']\n",
      "\n",
      "üìÑ test_index.parquet\n",
      "Shape: (226362, 5)\n",
      "Columnas:\n",
      "['fecha_compra', 'ticket_id', 'item_id', 'customer_id', 'devuelto']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "base_path = \"data/processed/devoluciones/\"\n",
    "\n",
    "files = [\n",
    "    \"X_train.parquet\",\n",
    "    \"X_test.parquet\",\n",
    "    \"y_train.parquet\",\n",
    "    \"y_test.parquet\",\n",
    "    \"train_index.parquet\",\n",
    "    \"test_index.parquet\"\n",
    "]\n",
    "\n",
    "for f in files:\n",
    "    df = pd.read_parquet(base_path + f)\n",
    "    print(f\"\\nüìÑ {f}\")\n",
    "    print(\"Shape:\", df.shape)\n",
    "    print(\"Columnas:\")\n",
    "    print(df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae1b0ab5-df84-4b8e-9142-df2ff9e5f7f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Guardado modelos/devoluciones/feature_columns.json con 83 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PEDRO\\AppData\\Local\\Temp\\ipykernel_12044\\2463886842.py:43: DtypeWarning: Columns (3,5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(PATH_ITEMS_AJUSTADAS_CSV)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Cargado data/items_devoluciones_ajustadas.csv | shape=(905445, 29)\n",
      "\n",
      "‚úÖ items_model_global_enriched creado\n",
      "Parquet: data/bi\\items_model_global_enriched.parquet\n",
      "CSV: data/bi\\items_model_global_enriched.csv\n",
      "Shape final: (905445, 33)\n",
      "‚ÑπÔ∏è % filas sin categoria tras merge: 0.00%\n",
      "\n",
      "Preview:\n",
      "  fecha_compra  fecha_item      item_id          sku id_producto categoria  \\\n",
      "0   2017-08-01  2017-08-01  T000001-001   P005-NAV-L        P005    Abrigo   \n",
      "1   2017-08-01  2017-08-01  T000002-001   P005-BEI-L        P005    Abrigo   \n",
      "2   2017-08-01  2017-08-01  T000003-001  P002-BLU-XL        P002  Camiseta   \n",
      "\n",
      "    canal provincia color talla  ...  zona_logistica  altura_cm  peso_kg  \\\n",
      "0  online   granada   LME     L  ...              Z2      184.2     71.2   \n",
      "1  online  gipuzkoa   WHT     L  ...              Z1      182.3     75.4   \n",
      "2  online    madrid   RED    XL  ...              Z1      189.6     94.5   \n",
      "\n",
      "     bmi  pvp_unitario  descuento_pct  coste_bruto margen_unit  \\\n",
      "0  20.99          95.0           0.05         45.0       45.25   \n",
      "1  22.68          95.0           0.00         45.0       50.00   \n",
      "2  26.29          26.0           0.00         10.0       16.00   \n",
      "\n",
      "  dias_hasta_devolucion fecha_devolucion  \n",
      "0                   NaN              NaN  \n",
      "1                   NaN              NaN  \n",
      "2                   NaN              NaN  \n",
      "\n",
      "[3 rows x 33 columns]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "PATH_X_TRAIN = \"data/processed/devoluciones/X_train.parquet\"\n",
    "PATH_X_TEST  = \"data/processed/devoluciones/X_test.parquet\"\n",
    "PATH_I_TRAIN = \"data/processed/devoluciones/train_index.parquet\"\n",
    "PATH_I_TEST  = \"data/processed/devoluciones/test_index.parquet\"\n",
    "\n",
    "PATH_MODEL   = \"modelos/devoluciones/xgb_final.json\"\n",
    "\n",
    "# ‚úÖ TU TABLA \"ENRICHED\" REAL (elige la que exista)\n",
    "# Si est√° en /data directamente, prueba esto:\n",
    "PATH_ITEMS_AJUSTADAS_PARQUET = \"data/items_devoluciones_ajustadas.parquet\"\n",
    "PATH_ITEMS_AJUSTADAS_CSV     = \"data/items_devoluciones_ajustadas.csv\"\n",
    "\n",
    "OUT_DIR = \"data/bi\"\n",
    "OUT_PARQUET = os.path.join(OUT_DIR, \"items_model_global_enriched.parquet\")\n",
    "OUT_CSV     = os.path.join(OUT_DIR, \"items_model_global_enriched.csv\")\n",
    "\n",
    "FEATURES_JSON = \"modelos/devoluciones/feature_columns.json\"\n",
    "\n",
    "\n",
    "# =========================\n",
    "# HELPERS\n",
    "# =========================\n",
    "def ensure_dir(path: str) -> None:\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "\n",
    "def load_items_ajustadas() -> pd.DataFrame:\n",
    "    \"\"\"Carga items_devoluciones_ajustadas desde parquet o csv.\"\"\"\n",
    "    if os.path.exists(PATH_ITEMS_AJUSTADAS_PARQUET):\n",
    "        df = pd.read_parquet(PATH_ITEMS_AJUSTADAS_PARQUET)\n",
    "        print(f\"‚úÖ Cargado {PATH_ITEMS_AJUSTADAS_PARQUET} | shape={df.shape}\")\n",
    "        return df\n",
    "\n",
    "    if os.path.exists(PATH_ITEMS_AJUSTADAS_CSV):\n",
    "        df = pd.read_csv(PATH_ITEMS_AJUSTADAS_CSV)\n",
    "        print(f\"‚úÖ Cargado {PATH_ITEMS_AJUSTADAS_CSV} | shape={df.shape}\")\n",
    "        return df\n",
    "\n",
    "    raise FileNotFoundError(\n",
    "        \"‚ùå No encuentro items_devoluciones_ajustadas.\\n\"\n",
    "        f\"Busqu√©:\\n- {PATH_ITEMS_AJUSTADAS_PARQUET}\\n- {PATH_ITEMS_AJUSTADAS_CSV}\\n\"\n",
    "        \"Soluci√≥n: revisa el nombre exacto del archivo o su ruta.\"\n",
    "    )\n",
    "\n",
    "\n",
    "def load_and_check():\n",
    "    X_train = pd.read_parquet(PATH_X_TRAIN)\n",
    "    X_test  = pd.read_parquet(PATH_X_TEST)\n",
    "\n",
    "    idx_train = pd.read_parquet(PATH_I_TRAIN)\n",
    "    idx_test  = pd.read_parquet(PATH_I_TEST)\n",
    "\n",
    "    # Columnas id√©nticas en train/test\n",
    "    if list(X_train.columns) != list(X_test.columns):\n",
    "        raise ValueError(\"‚ùå X_train y X_test no tienen las mismas columnas en el mismo orden.\")\n",
    "\n",
    "    # Tama√±os coinciden con √≠ndices\n",
    "    if len(X_train) != len(idx_train):\n",
    "        raise ValueError(f\"‚ùå X_train ({len(X_train)}) y train_index ({len(idx_train)}) no coinciden.\")\n",
    "    if len(X_test) != len(idx_test):\n",
    "        raise ValueError(f\"‚ùå X_test ({len(X_test)}) y test_index ({len(idx_test)}) no coinciden.\")\n",
    "\n",
    "    return X_train, X_test, idx_train, idx_test\n",
    "\n",
    "\n",
    "def predict_proba(model_path: str, X: pd.DataFrame) -> pd.Series:\n",
    "    booster = xgb.Booster()\n",
    "    booster.load_model(model_path)\n",
    "\n",
    "    dmat = xgb.DMatrix(X, feature_names=list(X.columns))\n",
    "    p = booster.predict(dmat)\n",
    "    return pd.Series(p, index=X.index, name=\"p_dev_global\")\n",
    "\n",
    "\n",
    "def keep_existing_cols(df: pd.DataFrame, cols: list[str]) -> list[str]:\n",
    "    return [c for c in cols if c in df.columns]\n",
    "\n",
    "\n",
    "# =========================\n",
    "# MAIN\n",
    "# =========================\n",
    "def main():\n",
    "    ensure_dir(OUT_DIR)\n",
    "    ensure_dir(os.path.dirname(FEATURES_JSON))\n",
    "\n",
    "    # 1) Cargar X + indices\n",
    "    X_train, X_test, idx_train, idx_test = load_and_check()\n",
    "\n",
    "    # 2) Guardar features (√∫til para siempre)\n",
    "    with open(FEATURES_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(list(X_train.columns), f, ensure_ascii=False, indent=2)\n",
    "    print(f\"‚úÖ Guardado {FEATURES_JSON} con {X_train.shape[1]} features\")\n",
    "\n",
    "    # 3) Predecir\n",
    "    p_train = predict_proba(PATH_MODEL, X_train)\n",
    "    p_test  = predict_proba(PATH_MODEL, X_test)\n",
    "\n",
    "    # 4) Tabla diagn√≥stico b√°sica (ids + prob)\n",
    "    train_out = idx_train.copy()\n",
    "    train_out[\"p_dev_global\"] = p_train.values\n",
    "\n",
    "    test_out = idx_test.copy()\n",
    "    test_out[\"p_dev_global\"] = p_test.values\n",
    "\n",
    "    out = pd.concat([train_out, test_out], ignore_index=True)\n",
    "\n",
    "    # Renombrar objetivo para BI\n",
    "    if \"devuelto\" in out.columns:\n",
    "        out = out.rename(columns={\"devuelto\": \"devuelto_real\"})\n",
    "\n",
    "    # 5) Coste base y expected cost\n",
    "    # Usamos el precio_neto que se us√≥ en el modelo (feature)\n",
    "    precio_neto_train = X_train[\"precio_neto\"].reset_index(drop=True)\n",
    "    precio_neto_test  = X_test[\"precio_neto\"].reset_index(drop=True)\n",
    "    out[\"precio_neto_model\"] = pd.concat([precio_neto_train, precio_neto_test], ignore_index=True).clip(lower=0)\n",
    "\n",
    "    out[\"expected_cost_global\"] = out[\"p_dev_global\"] * out[\"precio_neto_model\"]\n",
    "\n",
    "    # 6) Cargar tu tabla ‚Äúenriched real‚Äù (items_devoluciones_ajustadas)\n",
    "    items = load_items_ajustadas()\n",
    "\n",
    "    if \"item_id\" not in items.columns:\n",
    "        raise ValueError(\"‚ùå items_devoluciones_ajustadas no tiene 'item_id'.\")\n",
    "\n",
    "    # Evitar duplicados por item_id (merge seguro)\n",
    "    dup = items[\"item_id\"].duplicated().sum()\n",
    "    if dup > 0:\n",
    "        print(f\"‚ö†Ô∏è Ojo: hay {dup} item_id duplicados en items_devoluciones_ajustadas. Me quedo con el primero.\")\n",
    "        items = items.drop_duplicates(\"item_id\", keep=\"first\")\n",
    "\n",
    "    # 7) Seleccionar dimensiones √∫tiles (las que existan)\n",
    "    wanted_dims = [\n",
    "        \"item_id\", \"ticket_id\", \"customer_id\",\n",
    "        \"canal\", \"provincia\", \"provincia_norm\", \"canal_norm\", \"zona_logistica\",\n",
    "        \"fecha_item\",\n",
    "        \"sku\", \"id_producto\", \"categoria\", \"color\", \"talla\",\n",
    "        \"altura_cm\", \"peso_kg\", \"bmi\",\n",
    "        \"pvp_unitario\", \"descuento_pct\", \"precio_neto_unit\",\n",
    "        \"coste_bruto\", \"margen_unit\",\n",
    "        \"coste_devolucion\", \"dias_hasta_devolucion\", \"fecha_devolucion\",\n",
    "        \"devuelto\"\n",
    "    ]\n",
    "    dim_cols = keep_existing_cols(items, wanted_dims)\n",
    "    dims = items[dim_cols].copy()\n",
    "\n",
    "    # Si en items tambi√©n existe devuelto, lo renombramos para no confundir\n",
    "    if \"devuelto\" in dims.columns:\n",
    "        dims = dims.rename(columns={\"devuelto\": \"devuelto_items\"})\n",
    "\n",
    "    # 8) Merge (out = many rows, dims = 1 por item_id)\n",
    "    final = out.merge(dims, on=\"item_id\", how=\"left\", validate=\"many_to_one\")\n",
    "\n",
    "    # 9) Reordenar columnas (BI-friendly)\n",
    "    first_cols = [\n",
    "        \"fecha_compra\", \"fecha_item\",\n",
    "        \"ticket_id\", \"item_id\", \"customer_id\",\n",
    "        \"sku\", \"id_producto\", \"categoria\", \"canal\", \"provincia\",\n",
    "        \"color\", \"talla\",\n",
    "        \"devuelto_real\", \"devuelto_items\",\n",
    "        \"p_dev_global\", \"precio_neto_model\", \"precio_neto_unit\",\n",
    "        \"expected_cost_global\",\n",
    "        \"coste_devolucion\"\n",
    "    ]\n",
    "    first_cols = [c for c in first_cols if c in final.columns]\n",
    "    rest_cols = [c for c in final.columns if c not in first_cols]\n",
    "    final = final[first_cols + rest_cols]\n",
    "\n",
    "    # 10) Guardar\n",
    "    final.to_parquet(OUT_PARQUET, index=False)\n",
    "    final.to_csv(OUT_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    print(\"\\n‚úÖ items_model_global_enriched creado\")\n",
    "    print(\"Parquet:\", OUT_PARQUET)\n",
    "    print(\"CSV:\", OUT_CSV)\n",
    "    print(\"Shape final:\", final.shape)\n",
    "\n",
    "    # Checks r√°pidos\n",
    "    if \"categoria\" in final.columns:\n",
    "        pct_missing_cat = final[\"categoria\"].isna().mean()\n",
    "        print(f\"‚ÑπÔ∏è % filas sin categoria tras merge: {pct_missing_cat:.2%}\")\n",
    "\n",
    "    print(\"\\nPreview:\")\n",
    "    print(final.head(3))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ropa)",
   "language": "python",
   "name": "ropa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
